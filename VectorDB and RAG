import os
import faiss
import numpy as np
import textwrap
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# -------------------------
# Step 1: Load text files
# -------------------------
def load_documents(folder_path):
    documents = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".txt"):
            with open(os.path.join(folder_path, filename), "r", encoding="utf-8") as f:
                text = f.read()
                documents.append({"filename": filename, "content": text})
    return documents

# -------------------------
# Step 2: Preprocess & Chunk text
# -------------------------
def chunk_text(text, chunk_size=500, overlap=50):
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = min(start + chunk_size, len(words))
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks

# -------------------------
# Step 3: Create embeddings
# -------------------------
def create_embeddings(docs, model_name="sentence-transformers/all-MiniLM-L6-v2"):
    model = SentenceTransformer(model_name)
    embeddings, metadata = [], []
    
    for doc in docs:
        chunks = chunk_text(doc["content"])
        for i, chunk in enumerate(chunks):
            vector = model.encode(chunk)
            embeddings.append(vector)
            metadata.append({
                "filename": doc["filename"],
                "chunk_id": i,
                "content": chunk
            })
    return np.array(embeddings), metadata, model

# -------------------------
# Step 4: Build FAISS Index
# -------------------------
def build_faiss_index(embeddings):
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index

# -------------------------
# Step 5: Query Function
# -------------------------
def search(query, model, index, metadata, top_k=5):
    query_vec = model.encode([query])
    D, I = index.search(np.array(query_vec), k=top_k)
    results = []
    for idx in I[0]:
        results.append(metadata[idx])
    return results

# -------------------------
# Step 6: Generate Answer with LexT5 (Optimized)
# -------------------------
def generate_answer(query, context_chunks, tokenizer, model, device):
    # LexT5 is trained for summarization, so we format it accordingly
    # Combine context into a coherent document
    context_text = "\n".join([c['content'][:400] for c in context_chunks[:3]])
    
    # LexT5 models work best with direct summarization prompts
    # Format: "summarize: <text>" or "question: <q> context: <c>"
    prompt = f"question: {query} context: {context_text}"
    
    # Tokenize with appropriate length for legal text
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=1024,  # LexT5 can handle longer context
        truncation=True,
        padding=True
    ).to(device)
    
    # Generate with settings optimized for legal text
    with torch.no_grad():  # Disable gradient computation for speed
        outputs = model.generate(
            **inputs,
            max_length=200,      # Reasonable answer length
            min_length=20,       # Ensure substantial answer
            num_beams=3,         # Balance quality/speed
            length_penalty=1.0,  # Neutral length preference
            early_stopping=True,
            no_repeat_ngram_size=3
        )
    
    # Decode the response
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

# -------------------------
# Main Script
# -------------------------
if __name__ == "__main__":
    folder = "legal_docs"  # <-- put your 200 txt files here
    documents = load_documents(folder)
    
    print(f"ðŸ“‚ Loaded {len(documents)} legal documents.")
    
    # Create embeddings
    embeddings, metadata, embedding_model = create_embeddings(documents)
    
    # Build FAISS index
    index = build_faiss_index(embeddings)
    print(f"âœ… Indexed {len(metadata)} text chunks.")
    
    # Load LexT5 model for answer generation
    print("ðŸ”„ Loading LexT5 model...")
    
    # Choose model based on your computational resources:
    # "pszemraj/LexT5-small" - 60M params (fastest, good for CPU)
    # "pszemraj/LexT5-base" - 220M params (balanced)
    # "pszemraj/LexT5-large" - 770M params (best quality, needs GPU)
    
    model_name = "pszemraj/LexT5-small"  # Change this to base or large if needed
    
    # Detect device (GPU if available, else CPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ðŸ’» Using device: {device}")
    print(f"ðŸ“¦ Model: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    llm_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    llm_model.to(device)  # Move model to GPU if available
    llm_model.eval()  # Set to evaluation mode for faster inference
    print("âœ… Model loaded successfully!")
    
    # Interactive Query Loop
    print("\nâš–ï¸ Legal RAG Assistant is ready! Type your query below.")
    print("Type 'exit' to quit.\n")
    
    while True:
        query = input("â“ Your Query: ")
        if query.lower() in ["exit", "quit"]:
            print("ðŸ‘‹ Exiting Legal Research Assistant. Goodbye!")
            break
        
        # Retrieve top chunks
        results = search(query, embedding_model, index, metadata, top_k=5)
        
        # Show retrieved chunks (optional)
        print("\nðŸ“Œ Retrieved Context Chunks:")
        for r in results:
            print(f"ðŸ“„ File: {r['filename']} | Chunk: {r['chunk_id']}")
            print(textwrap.fill(r['content'], width=80))
            print("-" * 80)
        
        # Generate final answer with LexT5
        answer = generate_answer(query, results, tokenizer, llm_model, device)
        print("\nðŸ¤– Assistant Answer:\n")
        print(textwrap.fill(answer, width=90))
        print("=" * 90)