{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44edde3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m score\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from bert_score import score\n",
    "import spacy, re\n",
    "from summac.model_summac import SummaCZS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"legal_summaries.csv\")\n",
    "# Columns: ['id', 'reference_summary', 'generated_summary', 'source_text']\n",
    "\n",
    "references = df['reference_summary'].tolist()\n",
    "predictions = df['generated_summary'].tolist()\n",
    "sources = df['source_text'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load(\"rouge\")\n",
    "meteor = load(\"meteor\")\n",
    "\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "meteor_results = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"ROUGE:\", rouge_results)\n",
    "print(\"METEOR:\", meteor_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTScore (using LegalBERT)\n",
    "P, R, F1 = score(predictions, references, lang=\"en\", model_type=\"nlpaueb/legal-bert-base-uncased\")\n",
    "print(\"BERTScore F1:\", F1.mean().item())\n",
    "\n",
    "# BARTScore\n",
    "from bart_score import BARTScorer\n",
    "bart_scorer = BARTScorer(device='cuda' if torch.cuda.is_available() else 'cpu', checkpoint='facebook/bart-large-cnn')\n",
    "\n",
    "bart_scores = bart_scorer.score(predictions, references, batch_size=4)\n",
    "print(\"BARTScore Average:\", sum(bart_scores)/len(bart_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SummaCZS(granularity=\"paragraph\", model_name=\"vitc\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "factual_scores = []\n",
    "\n",
    "for src, pred in zip(sources, predictions):\n",
    "    factuality = model.score([src], [pred])\n",
    "    factual_scores.append(factuality['scores'][0])\n",
    "\n",
    "df['factuality'] = factual_scores\n",
    "print(\"Average factuality:\", df['factuality'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def entity_precision(ref, pred):\n",
    "    ref_ents = {ent.text for ent in nlp(ref).ents}\n",
    "    pred_ents = {ent.text for ent in nlp(pred).ents}\n",
    "    if not pred_ents: return 1.0 if not ref_ents else 0.0\n",
    "    correct = len(pred_ents & ref_ents)\n",
    "    return correct / len(pred_ents)\n",
    "\n",
    "df['NEPrec'] = [entity_precision(r, p) for r, p in zip(references, predictions)]\n",
    "print(\"Average NEPrec:\", df['NEPrec'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b181e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_precision(ref, pred):\n",
    "    ref_nums = set(re.findall(r'\\d+', ref))\n",
    "    pred_nums = set(re.findall(r'\\d+', pred))\n",
    "    if not pred_nums: return 1.0 if not ref_nums else 0.0\n",
    "    correct = len(pred_nums & ref_nums)\n",
    "    return correct / len(pred_nums)\n",
    "\n",
    "df['NumPrec'] = [numeric_precision(r, p) for r, p in zip(references, predictions)]\n",
    "print(\"Average NumPrec:\", df['NumPrec'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c28998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ROUGE_L'] = rouge.compute(predictions=predictions, references=references)['rougeL']\n",
    "df['BERT_F1'] = [f.item() for f in F1]\n",
    "\n",
    "summary_metrics = {\n",
    "    \"ROUGE-L\": df['ROUGE_L'].mean(),\n",
    "    \"METEOR\": meteor_results['meteor'],\n",
    "    \"BERTScore-F1\": df['BERT_F1'].mean(),\n",
    "    \"BARTScore\": sum(bart_scores)/len(bart_scores),\n",
    "    \"Factuality\": df['factuality'].mean(),\n",
    "    \"NEPrec\": df['NEPrec'].mean(),\n",
    "    \"NumPrec\": df['NumPrec'].mean()\n",
    "}\n",
    "\n",
    "print(pd.DataFrame(summary_metrics, index=[\"Hybrid Model\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame([\n",
    "    {\"Model\": \"Extractive\", \"ROUGE-L\": 0.74, \"BERTScore-F1\": 0.82, \"Factuality\": 0.91, \"NEPrec\": 0.83},\n",
    "    {\"Model\": \"Abstractive\", \"ROUGE-L\": 0.68, \"BERTScore-F1\": 0.88, \"Factuality\": 0.77, \"NEPrec\": 0.80},\n",
    "    {\"Model\": \"Hybrid\", \"ROUGE-L\": 0.81, \"BERTScore-F1\": 0.90, \"Factuality\": 0.92, \"NEPrec\": 0.89},\n",
    "])\n",
    "\n",
    "print(comparison)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
