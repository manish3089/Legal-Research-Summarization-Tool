{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c44edde3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The pyarrow installation is not built with support for the Parquet file format (DLL load failed while importing _parquet: The specified module could not be found.)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATASETS_BACKEND\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      6\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCJWeiss/ZeroLexSumm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_billsum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.3.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_dataset.py:77\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\arrow_reader.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Optional, Union\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\parquet\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# distributed with this work for additional information\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\parquet\\core.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_parquet\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pyarrow installation is not built with support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the Parquet file format (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(exc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ParquetReader, Statistics,  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     40\u001b[0m                               FileMetaData, RowGroupMetaData,\n\u001b[0;32m     41\u001b[0m                               ColumnChunkMetaData,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m                               FileDecryptionProperties,\n\u001b[0;32m     46\u001b[0m                               SortingColumn)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (LocalFileSystem, FileType, _resolve_filesystem_and_path,\n\u001b[0;32m     48\u001b[0m                         _ensure_filesystem)\n",
      "\u001b[1;31mImportError\u001b[0m: The pyarrow installation is not built with support for the Parquet file format (DLL load failed while importing _parquet: The specified module could not be found.)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"DATASETS_BACKEND\"] = \"fastparquet\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"CJWeiss/ZeroLexSumm\", \"zero_billsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c443f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ZeroLexSumm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mZeroLexSumm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m      3\u001b[0m ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCJWeiss/ZeroLexSumm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_inabs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ZeroLexSumm'"
     ]
    }
   ],
   "source": [
    "from ZeroLexSumm import load_dataset\n",
    "\n",
    "ds = load_dataset(\"CJWeiss/ZeroLexSumm\", \"zero_inabs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"legal_summaries.csv\")\n",
    "# Columns: ['id', 'reference_summary', 'generated_summary', 'source_text']\n",
    "\n",
    "references = df['reference_summary'].tolist()\n",
    "predictions = df['generated_summary'].tolist()\n",
    "sources = df['source_text'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load(\"rouge\")\n",
    "meteor = load(\"meteor\")\n",
    "\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "meteor_results = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"ROUGE:\", rouge_results)\n",
    "print(\"METEOR:\", meteor_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTScore (using LegalBERT)\n",
    "P, R, F1 = score(predictions, references, lang=\"en\", model_type=\"nlpaueb/legal-bert-base-uncased\")\n",
    "print(\"BERTScore F1:\", F1.mean().item())\n",
    "\n",
    "# BARTScore\n",
    "from bart_score import BARTScorer\n",
    "bart_scorer = BARTScorer(device='cuda' if torch.cuda.is_available() else 'cpu', checkpoint='facebook/bart-large-cnn')\n",
    "\n",
    "bart_scores = bart_scorer.score(predictions, references, batch_size=4)\n",
    "print(\"BARTScore Average:\", sum(bart_scores)/len(bart_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SummaCZS(granularity=\"paragraph\", model_name=\"vitc\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "factual_scores = []\n",
    "\n",
    "for src, pred in zip(sources, predictions):\n",
    "    factuality = model.score([src], [pred])\n",
    "    factual_scores.append(factuality['scores'][0])\n",
    "\n",
    "df['factuality'] = factual_scores\n",
    "print(\"Average factuality:\", df['factuality'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def entity_precision(ref, pred):\n",
    "    ref_ents = {ent.text for ent in nlp(ref).ents}\n",
    "    pred_ents = {ent.text for ent in nlp(pred).ents}\n",
    "    if not pred_ents: return 1.0 if not ref_ents else 0.0\n",
    "    correct = len(pred_ents & ref_ents)\n",
    "    return correct / len(pred_ents)\n",
    "\n",
    "df['NEPrec'] = [entity_precision(r, p) for r, p in zip(references, predictions)]\n",
    "print(\"Average NEPrec:\", df['NEPrec'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b181e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_precision(ref, pred):\n",
    "    ref_nums = set(re.findall(r'\\d+', ref))\n",
    "    pred_nums = set(re.findall(r'\\d+', pred))\n",
    "    if not pred_nums: return 1.0 if not ref_nums else 0.0\n",
    "    correct = len(pred_nums & ref_nums)\n",
    "    return correct / len(pred_nums)\n",
    "\n",
    "df['NumPrec'] = [numeric_precision(r, p) for r, p in zip(references, predictions)]\n",
    "print(\"Average NumPrec:\", df['NumPrec'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c28998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ROUGE_L'] = rouge.compute(predictions=predictions, references=references)['rougeL']\n",
    "df['BERT_F1'] = [f.item() for f in F1]\n",
    "\n",
    "summary_metrics = {\n",
    "    \"ROUGE-L\": df['ROUGE_L'].mean(),\n",
    "    \"METEOR\": meteor_results['meteor'],\n",
    "    \"BERTScore-F1\": df['BERT_F1'].mean(),\n",
    "    \"BARTScore\": sum(bart_scores)/len(bart_scores),\n",
    "    \"Factuality\": df['factuality'].mean(),\n",
    "    \"NEPrec\": df['NEPrec'].mean(),\n",
    "    \"NumPrec\": df['NumPrec'].mean()\n",
    "}\n",
    "\n",
    "print(pd.DataFrame(summary_metrics, index=[\"Hybrid Model\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733ac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame([\n",
    "    {\"Model\": \"Extractive\", \"ROUGE-L\": 0.74, \"BERTScore-F1\": 0.82, \"Factuality\": 0.91, \"NEPrec\": 0.83},\n",
    "    {\"Model\": \"Abstractive\", \"ROUGE-L\": 0.68, \"BERTScore-F1\": 0.88, \"Factuality\": 0.77, \"NEPrec\": 0.80},\n",
    "    {\"Model\": \"Hybrid\", \"ROUGE-L\": 0.81, \"BERTScore-F1\": 0.90, \"Factuality\": 0.92, \"NEPrec\": 0.89},\n",
    "])\n",
    "\n",
    "print(comparison)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
